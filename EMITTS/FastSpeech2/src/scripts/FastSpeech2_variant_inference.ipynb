{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastSpeech 2 Variant inferendce code\n",
    "\n",
    "Edit the variables **checkpoint_path**, **text** and **emotion** to match yours and run the entire code to generate wav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "src_path = os.path.join(\"/\", *os.getcwd().split(os.sep)[:-2])\n",
    "sys.path.append(src_path)\n",
    "os.chdir(src_path)\n",
    "from src.dataset.dataset import Dataset\n",
    "from config.config import TrainConfig\n",
    "import torch\n",
    "from lightning import seed_everything\n",
    "from src.models import Generator, TorchSTFT\n",
    "from dataclasses import asdict\n",
    "from src.models.acoustic_model.fastspeech.lightning_model import FastSpeechLightning\n",
    "from src.utils.vocoder_utils import load_checkpoint, synthesize_wav_from_mel\n",
    "import IPython.display as ipd\n",
    "\n",
    "PROJECT_PATH = os.path.join(\"/\", *os.getcwd().split(os.sep)[:-2])\n",
    "config = TrainConfig()\n",
    "DATASET = \"ESD\"\n",
    "# DATASET = \"MEADTTS\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set checkpoint path, e.g. \"ckpt/ESD/model.ckpt\"\n",
    "config.testing_checkpoint = f\"{PROJECT_PATH}/EMITTS/FastSpeech2/ckpt/{DATASET}/model.ckpt\"\n",
    "# config.testing_checkpoint = f\"{PROJECT_PATH}/EMITTS/FastSpeech2/ckpt/MEADTTS/epoch=559-step=217280.ckpt\"\n",
    "config.phones_path = \"../../data/MEADTTS_MFA_preprocessed/phones.json\"\n",
    "\n",
    "def load_data(datalist):\n",
    "    with open(datalist, encoding='utf-8') as f:\n",
    "        data = [line.strip().split(\"|\") for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Dataset(\n",
    "        filename=\"test.txt\", cfg=config, batch_size=config.val_batch_size, is_emotion_feature=config.is_emotion_feature\n",
    "    )\n",
    "test_sample = test_data[np.random.randint(0, len(test_data))]\n",
    "test_data_text = load_data(f\"../filelist/{DATASET}/esd_en_audio_sid_text_efeature_test_filelist.txt\")\n",
    "# test_data_text = load_data(f\"../filelist/{DATASET}/MEADTTS_audio_sid_text_efeature_test_filelist.txt\")\n",
    "test_df = pd.DataFrame(test_data_text, columns=[\"file_name\", \"speaker\", \"text\", \"emotion_feature\"])\n",
    "test_item = test_df[test_df[\"file_name\"].str.contains(test_sample[\"id\"])]\n",
    "\n",
    "# MEADTTS\n",
    "# test_id = \"\"\n",
    "# if test_sample[\"id\"][1] == \"_\":\n",
    "#     test_id = \"W\" + str(int(test_sample[\"id\"][:1])).zfill(3) + test_sample[\"id\"][1:]\n",
    "# elif int(test_sample[\"id\"][:2]) <= 40:\n",
    "#     test_id = \"W\" + str(int(test_sample[\"id\"][:2])).zfill(3) + test_sample[\"id\"][2:]\n",
    "# else:\n",
    "#     test_id = \"M\" + str(int(test_sample[\"id\"][:2]) - 40).zfill(3) + test_sample[\"id\"][2:]\n",
    "\n",
    "# test_item = test_df[test_df[\"file_name\"].str.contains(test_id)]\n",
    "\n",
    "test_item[\"text\"].values[0], test_item[\"speaker\"].values[0], test_item[\"emotion_feature\"].values[0].split(\"/\")[-1][:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_input = {\n",
    "    \"ids\": [test_sample[\"id\"]],\n",
    "    \"speakers\": torch.Tensor([test_sample[\"speaker\"]]).long(),\n",
    "    \"emotions\": torch.Tensor([test_sample[\"emotion\"]]).long(),\n",
    "    \"texts\": torch.Tensor([test_sample[\"text\"]]).long(),\n",
    "    \"text_lens\": torch.Tensor([len(test_sample[\"text\"])]).long(),\n",
    "    \"mels\": None,\n",
    "    \"mel_lens\": None,\n",
    "    \"pitches\": None,\n",
    "    \"energies\": None,\n",
    "    \"durations\": None,\n",
    "    \"egemap_features\": None,\n",
    "    \"emotion_features\": torch.Tensor(test_sample[\"emotion_feature\"]).float().unsqueeze(0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config.seed)\n",
    "vocoder = Generator(**asdict(config))\n",
    "stft = TorchSTFT(**asdict(config))\n",
    "vocoder_state_dict = load_checkpoint(config.vocoder_checkpoint_path)\n",
    "vocoder.load_state_dict(vocoder_state_dict[\"generator\"])\n",
    "vocoder.remove_weight_norm()\n",
    "vocoder.eval()\n",
    "model = FastSpeechLightning.load_from_checkpoint(\n",
    "    config.testing_checkpoint,\n",
    "    config=config,\n",
    "    vocoder=vocoder,\n",
    "    stft=stft,\n",
    "    train=False,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model.model(model.device, test_sample_input)\n",
    "    predicted_mel_len = model_output[\"mel_len\"][0]\n",
    "    predicted_mel_no_padding = model_output[\"predicted_mel\"][0, :predicted_mel_len]\n",
    "    generated_wav = synthesize_wav_from_mel(\n",
    "        predicted_mel_no_padding, model.vocoder, model.stft\n",
    "    )\n",
    "ipd.Audio(generated_wav, rate=config.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for sample in tqdm(test_data):\n",
    "    sample_input = {\n",
    "        \"ids\": [sample[\"id\"]],\n",
    "        \"speakers\": torch.Tensor([sample[\"speaker\"]]).long(),\n",
    "        \"emotions\": torch.Tensor([sample[\"emotion\"]]).long(),\n",
    "        \"texts\": torch.Tensor([sample[\"text\"]]).long(),\n",
    "        \"text_lens\": torch.Tensor([len(sample[\"text\"])]).long(),\n",
    "        \"mels\": None,\n",
    "        \"mel_lens\": None,\n",
    "        \"pitches\": None,\n",
    "        \"energies\": None,\n",
    "        \"durations\": None,\n",
    "        \"egemap_features\": None,\n",
    "        \"emotion_features\": torch.Tensor(sample[\"emotion_feature\"]).float().unsqueeze(0),\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        model_output = model.model(model.device, sample_input)\n",
    "        predicted_mel_len = model_output[\"mel_len\"][0]\n",
    "        predicted_mel_no_padding = model_output[\"predicted_mel\"][0, :predicted_mel_len]\n",
    "        generated_wav = synthesize_wav_from_mel(\n",
    "            predicted_mel_no_padding, model.vocoder, model.stft\n",
    "        )\n",
    "    ipd.Audio(generated_wav, rate=config.sample_rate)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
