{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VITS Variant inference code\n",
    "Edit the variables **checkpoint_path**, **text** and **emotion feature** to match yours and run the entire code to generate wav."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"/\", *os.getcwd().split(os.sep)))\n",
    "from utils.text_utils import text_to_sequence\n",
    "import torch\n",
    "import utils.commons as commons\n",
    "from utils.utils import get_hparams_from_file, load_checkpoint\n",
    "from model.models import SynthesizerTrn\n",
    "from utils.text.symbols import symbols\n",
    "import IPython.display as ipd\n",
    "\n",
    "PROJECT_PATH = os.path.join('/', *os.getcwd().split(os.sep)[:-2])\n",
    "# hps_file, hyperparams file, e.g. \"EMITTS/VITS/config/esd_en_e5.json\"\n",
    "hps_file = f\"{PROJECT_PATH}/EMITTS/VITS/config/esd_en_e5.json\"\n",
    "hps = get_hparams_from_file(hps_file)\n",
    "# checkpoint_path, checkpoint file path, e.g. \"EMITTS/VITS/ckpt/checkpoint.pth\"\n",
    "checkpoint_path = f\"{PROJECT_PATH}/EMITTS/VITS/ckpt/checkpoint.pth\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_text(text, hps):\n",
    "    text_norm = text_to_sequence(text, hps.data.text_cleaners)\n",
    "    if hps.data.add_blank:\n",
    "        text_norm = commons.intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SynthesizerTrn(\n",
    "            len(symbols),\n",
    "            hps.data.filter_length // 2 + 1,\n",
    "            hps.train.segment_size // hps.data.hop_length,\n",
    "            n_speakers=hps.data.n_speakers,\n",
    "            **hps.model).to(device).eval()\n",
    "load_checkpoint(checkpoint_path, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid, speaker id, e.g. 0\n",
    "sid = 4\n",
    "# text, text to synthesize, e.g. \"That I owe my thanks to you.\"\n",
    "text = \"That I owe my thanks to you.\"\n",
    "# emotion_f_path, emotion feature file path\n",
    "emotion_f_path = f\"{PROJECT_PATH}/EPAlign/mmefeature/tmp/explict/happy.pt\"\n",
    "\n",
    "sid = torch.LongTensor([sid]).to(device)\n",
    "text = get_text(text, hps).unsqueeze(0).to(device)\n",
    "text_length = torch.LongTensor([text.size(1)]).to(device)\n",
    "emotion_f = torch.load(emotion_f_path).float().unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    audio = model.infer(text, text_length, sid, emotion_f, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float()\n",
    "ipd.Audio(audio.numpy(), rate=hps.data.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umetts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
