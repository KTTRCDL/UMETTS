{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPAlign Prompt and Audio Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "from transformers import Wav2Vec2Processor\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DATASET is the dataset name model trained on, e.g. ESD, MELD\n",
    "DATASET = \"ESD\" # MELD\n",
    "\n",
    "# BATCH_SIZE should smaller/equal to the category of the emotion, e.g. for RAF-DB, the category is 7\n",
    "BATCH_SIZE = 5\n",
    "EPOCH = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_PATH = os.path.join('/', *os.getcwd().split(os.sep)[:-2])\n",
    "# PROCESSED_WAV2VEC2_PATH is the path to the Wav2Vec2Processor\n",
    "PROCESSED_WAV2VEC2_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base/wav2vec2\"\n",
    "# PRETRAIN_WAV2VEC2_PATH is the pretrained model path, e.g. EPAlign/ckpt/base/wav2vec2\n",
    "PRETRAIN_WAV2VEC2_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base/wav2vec2\"\n",
    "# ESD_FILELIST_PATH is the path to the ESD filelist\n",
    "ESD_FILELIST_PATH = f\"{PROJECT_PATH}/EMITTS/filelist/{DATASET}\"\n",
    "# PRETRAIN_CLIP_MODEL is the pretrained CLIP model, e.g. ViT-B-32\n",
    "PRETRAIN_CLIP_MODEL = \"ViT-B/32\"\n",
    "# PRETRAIN_CLIP_MODEL_PATH is the pretrained model path, e.g. EPAlign/ckpt/base\n",
    "PRETRAIN_CLIP_MODEL_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base\"\n",
    "# LOG_PATH is the log path, e.g. EPAlign/log\n",
    "LOG_PATH = f\"{PROJECT_PATH}/EPAlign/log\"\n",
    "# CKPT_PATH is the path to save checkpoint, e.g. EPAlign/ckpt/ESD\n",
    "CKPT_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Audio Model (consist of language model and acoustic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAP(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config, prompt_pretrain_model, prompt_pretrain_model_path):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "        width = 1024\n",
    "        scale = width ** -0.5\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, 512))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.prompt_model, self.prompt_processor = clip.load(prompt_pretrain_model, jit=False, download_root=prompt_pretrain_model_path)\n",
    "        self.prompt_model.to(device)\n",
    "    def forward(self, wavs, prompts):\n",
    "        audio_features = torch.tensor([]).to(device)\n",
    "        for wav in wavs:\n",
    "            audio_feature = self.wav2vec2(wav)\n",
    "            audio_feature = audio_feature[0]\n",
    "            audio_feature = torch.mean(audio_feature, dim=1)\n",
    "            audio_features = torch.cat((audio_features, audio_feature), dim=0)\n",
    "        audio_features = audio_features @ self.proj\n",
    "\n",
    "        prompt_features = clip.tokenize(prompts).to(device)\n",
    "        prompt_features = self.prompt_model.encode_text(prompt_features)\n",
    "        # normalized features\n",
    "        audio_features = audio_features / audio_features.norm(dim=1, keepdim=True)\n",
    "        prompt_features = prompt_features / prompt_features.norm(dim=1, keepdim=True)\n",
    "        audio_features = audio_features.float()\n",
    "        prompt_features = prompt_features.float()\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp().float()\n",
    "        logits_per_audio = logit_scale * audio_features @ prompt_features.t()\n",
    "        logits_per_text = logits_per_audio.t()\n",
    "        return logits_per_audio, logits_per_text\n",
    "\n",
    "model = CLAP.from_pretrained(PRETRAIN_WAV2VEC2_PATH, prompt_pretrain_model=PRETRAIN_CLIP_MODEL, prompt_pretrain_model_path=PRETRAIN_CLIP_MODEL_PATH).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Wav2Vec2 Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(PROCESSED_WAV2VEC2_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESDDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 datalist=\"path/to/datalist\",\n",
    "                 preprocess=None):\n",
    "        self.datalist = datalist\n",
    "        self.preprocess = preprocess\n",
    "        self.data = self.load_data()\n",
    "        self.text2label = {\n",
    "            \"angry\": 1,\n",
    "            \"happy\": 2,\n",
    "            \"neutral\": 3,\n",
    "            \"sad\": 4,\n",
    "            \"surprise\": 5,\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        wav_path = data[0]\n",
    "        wav, _ = librosa.load(wav_path, sr=16000)\n",
    "        # audio = torch.from_numpy(wav).float()\n",
    "        if self.preprocess is not None:\n",
    "            audio = self.preprocess(wav, sampling_rate=16000)\n",
    "            audio = audio[\"input_values\"][0]\n",
    "            audio = audio.reshape(1, -1)\n",
    "            audio = torch.from_numpy(audio).to(device).float()\n",
    "\n",
    "        prompt_feature_path = data[3]\n",
    "        emotiontag = prompt_feature_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        prompt = f\"A person speaking with a feeling of {emotiontag}\"\n",
    "        label = self.text2label[emotiontag]\n",
    "    \n",
    "        return audio, prompt, label\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open(self.datalist, encoding='utf-8') as f:\n",
    "            data = [line.strip().split(\"|\") for line in f]\n",
    "        return data\n",
    "    \n",
    "if DATASET == \"ESD\":\n",
    "    train_dataset = ESDDataset(datalist=f'{ESD_FILELIST_PATH}/esd_en_audio_sid_text_efeature_train_filelist.txt', preprocess=processor)\n",
    "    val_dataset = ESDDataset(datalist=f'{ESD_FILELIST_PATH}/esd_en_audio_sid_text_efeature_val_filelist.txt', preprocess=processor)\n",
    "    test_dataset = ESDDataset(datalist=f'{ESD_FILELIST_PATH}/esd_en_audio_sid_text_efeature_test_filelist.txt', preprocess=processor)\n",
    "\n",
    "assert len(train_dataset) == 14_000\n",
    "assert len(val_dataset) == 1_750\n",
    "assert len(test_dataset) == 1_750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Batch Sample (ensures no same class per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audios = [sample[0] for sample in batch]\n",
    "    prompts = [sample[1] for sample in batch]\n",
    "    labels = [sample[2] for sample in batch]\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "\n",
    "    return audios, prompts, labels\n",
    "\n",
    "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
    "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "test_labels = torch.tensor([item[2] for item in test_dataset])\n",
    "test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "loss_audio = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer = optim.Adam([model.proj, model.logit_scale], lr=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(f\"{LOG_PATH}/log_prompt_audio_{DATASET}.txt\")\n",
    "\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "log = logging.getLogger('')\n",
    "log.addHandler(file_handler)\n",
    "log.info('finetune start...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_te_loss = 1e5\n",
    "best_ep = -1\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for batch in pbar:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        audios, prompts, _ = batch\n",
    "        logits_per_audio, logits_per_text = model(audios, prompts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "        total_loss = (loss_audio(logits_per_audio,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        tr_loss += total_loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "    tr_loss /= step\n",
    "    \n",
    "    step = 0\n",
    "    te_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_pbar = tqdm(test_dataloader, leave=False)\n",
    "        for batch in test_pbar:\n",
    "            step += 1\n",
    "            audios, texts, _ = batch\n",
    "            logits_per_audio, logits_per_text = model(audios, texts)\n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "            total_loss = (loss_audio(logits_per_audio,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            te_loss += total_loss.item()\n",
    "            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
    "        te_loss /= step\n",
    "        \n",
    "    if te_loss < best_te_loss:\n",
    "        best_te_loss = te_loss\n",
    "        best_ep = epoch\n",
    "        torch.save(model.state_dict(), f\"{CKPT_PATH}/best_model_proj_logit.pt\")\n",
    "        torch.save(model.prompt_model.state_dict(), f\"{CKPT_PATH}/best_model.pt\")\n",
    "    print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "    # torch.save(model.state_dict(), f\"{CKPT_PATH}/ESD_ft_proj_logit_{epoch}_model.pt\")\n",
    "    # torch.save(model.prompt_model.state_dict(), f'{CKPT_PATH}/model_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
