{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPAlign Prompt and Vision Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# DATASET is the dataset name model trained on, e.g. RAF, MELD\n",
    "DATASET = \"RAF\" # MELD\n",
    "\n",
    "# BATCH_SIZE should smaller/equal to the category of the emotion, e.g. for RAF-DB, the category is 7\n",
    "BATCH_SIZE = 7\n",
    "EPOCH = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PROJECT_PATH = os.path.join('/', *os.getcwd().split(os.sep)[:-2])\n",
    "# PRETRAIN_MODEL is the pretrained model name, e.g. ViT-B/32\n",
    "PRETRAIN_MODEL = \"ViT-B/32\"\n",
    "# PRETRAIN_MODEL_PATH is the pretrained model path, e.g. EPAlign/ckpt/base\n",
    "PRETRAIN_MODEL_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/base\"\n",
    "# RAF_DATA_PATH is the RAF-DB dataset path, e.g. data/RAF/compound/Image/original\n",
    "RAF_DATA_PATH = f\"{PROJECT_PATH}/data/{DATASET}/compound/Image/original\"\n",
    "# RAF_LABEL_PATH is the RAF-DB label path, e.g. data/RAF/compound/EmoLabel/list_patition_label.txt\n",
    "RAF_LABEL_PATH = f\"{PROJECT_PATH}/data/{DATASET}/compound/EmoLabel/list_patition_label.txt\"\n",
    "# LOG_PATH is the log path, e.g. EPAlign/log\n",
    "LOG_PATH = f\"{PROJECT_PATH}/EPAlign/log\"\n",
    "# CKPT_PATH is the path to save checkpoint, e.g. EPAlign/ckpt/RAF\n",
    "CKPT_PATH = f\"{PROJECT_PATH}/EPAlign/ckpt/{DATASET}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(PRETRAIN_MODEL, device=device, jit=False, download_root=PRETRAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3162, 792)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAFDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_path=\"path/to/data\", \n",
    "                 mode=\"train\", \n",
    "                 datalist=\"path/to/label.txt\", \n",
    "                 preprocess=None):\n",
    "        self.data_path = data_path\n",
    "        self.mode = mode\n",
    "        self.datalist = datalist\n",
    "        self.preprocess = preprocess\n",
    "        self.data = self.load_data()\n",
    "        if self.datalist.find('compound') != -1:\n",
    "            self.label2text = { 1: \"Happily Surprised\", 2: \"Happily Disgusted\", 3: \"Sadly Fearful\", 4: \"Sadly Angry\", 5: \"Sadly Surprised\", 6: \"Sadly Disgusted\", 7: \"Fearfully Angry\", 8: \"Fearfully Surprised\", 9: \"Angrily Surprised\", 10: \"Angrily Disgusted\", 11: \"Disgustedly Surprised\"}\n",
    "        else:\n",
    "            self.label2text = { 1: \"Surprise\", 2: \"Fear\", 3: \"Disgust\", 4: \"Happiness\", 5: \"Sadness\", 6: \"Anger\", 7: \"Neutral\"}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data_path + '/' + self.data.iloc[idx][\"path\"])\n",
    "        if self.preprocess:\n",
    "            img = self.preprocess(img)\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "        text = self.label2text[int(label)]\n",
    "        return img, text ,label\n",
    "    \n",
    "    def load_data(self):\n",
    "        data = pd.read_csv(self.datalist, sep=\" \", header=None)\n",
    "        data.columns = [\"path\", \"label\"]\n",
    "        if self.mode == \"train\":\n",
    "            data = data[data[\"path\"].str.contains(\"train\")]\n",
    "        elif self.mode == \"test\":\n",
    "            data = data[data[\"path\"].str.contains(\"test\")]\n",
    "        else:\n",
    "            data = data[data[\"path\"].str.contains(\"test\")]\n",
    "        return data\n",
    "\n",
    "if DATASET == \"RAF\":\n",
    "    train_dataset = RAFDataset(mode='train', data_path=RAF_DATA_PATH, datalist=RAF_LABEL_PATH, preprocess=preprocess)\n",
    "    test_dataset = RAFDataset(mode='test', data_path=RAF_DATA_PATH, datalist=RAF_LABEL_PATH, preprocess=preprocess)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Batch Sample (ensures no same class per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "    \n",
    "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
    "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "test_labels = torch.tensor([item[2] for item in test_dataset])\n",
    "test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# import itertools\n",
    "# parameters = itertools.chain(model.visual.parameters(), [model.logit_scale])\n",
    "parameters = model.parameters()\n",
    "lr = 1e-5\n",
    "# betas = (0.9,0.98)\n",
    "# eps = 1e-6\n",
    "# weight_decay = 0.2\n",
    "\n",
    "# optimizer = optim.Adam(parameters, lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "optimizer = optim.Adam(parameters, lr=lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader) * EPOCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(f\"{LOG_PATH}/log_prompt_vision_{DATASET}.txt\")\n",
    "\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "log = logging.getLogger('')\n",
    "log.addHandler(file_handler)\n",
    "log.info('finetune start...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_te_loss = 1e5\n",
    "best_ep = -1\n",
    "for epoch in range(EPOCH):\n",
    "    log.info(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "    step = 0\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, leave=False)\n",
    "    for batch in pbar:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts, _ = batch\n",
    "        images = images.to(device)\n",
    "        texts = clip.tokenize(texts).to(device)\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        tr_loss += total_loss.item()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            clip.model.convert_weights(model)\n",
    "        pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "    tr_loss /= step\n",
    "    \n",
    "    step = 0\n",
    "    te_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_pbar = tqdm(test_dataloader, leave=False)\n",
    "        for batch in test_pbar:\n",
    "            step += 1\n",
    "            images, texts, _ = batch\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            te_loss += total_loss.item()\n",
    "            test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
    "        te_loss /= step\n",
    "        \n",
    "    if te_loss < best_te_loss:\n",
    "        best_te_loss = te_loss\n",
    "        best_ep = epoch\n",
    "        torch.save(model.state_dict(), f\"{CKPT_PATH}/best_model.pt\")\n",
    "    log.info(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "    # torch.save(model.state_dict(), f'{CKPT_PATH}/model_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
